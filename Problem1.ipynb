{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Problem1.ipynb","provenance":[],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"78065a500c4842249960dcf5185ed5d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ddc1d234ad604718852d0367068a55db","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cb359bc92c934a53b064c9722eaae8a4","IPY_MODEL_156aeecaac7c43ba8090ebab69ab33d5"]}},"ddc1d234ad604718852d0367068a55db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb359bc92c934a53b064c9722eaae8a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4d49299a5839457382aa7fbd365dc5a5","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_733d1bdfa5fd47bf8bd44461cdf5f668"}},"156aeecaac7c43ba8090ebab69ab33d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_30fb3a57161d42d6a32c2335fb6e9c36","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 500/500 [17:56&lt;00:00,  2.15s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d999bcb1d8b74f1abdd256e12bef9e98"}},"4d49299a5839457382aa7fbd365dc5a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"733d1bdfa5fd47bf8bd44461cdf5f668":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30fb3a57161d42d6a32c2335fb6e9c36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d999bcb1d8b74f1abdd256e12bef9e98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"LVSBOzak6Qu2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1598800109035,"user_tz":-600,"elapsed":5214,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"4708aee8-f512-47f3-dffc-657f7060cf10"},"source":["!pip3 install torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1ysjl7Qh6QvC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1598800118030,"user_tz":-600,"elapsed":4473,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"7a62b377-ef30-49c3-d0b2-78cfa3d1d98f"},"source":["!pip3 install torchvision"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n","Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GBh3bjSB6QvM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598800123253,"user_tz":-600,"elapsed":4074,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"a5db27e3-d1fd-45a3-b9c5-308e4882dc50"},"source":["!pip3 install tqdm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rxNH2vqc6QvT","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","from torch import optim\n","from torchvision.transforms import transforms\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","from tqdm.notebook import tqdm\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hg4M0Ll073El","colab_type":"code","colab":{}},"source":["import numpy as np\n","from os.path import join\n","from google.colab import drive\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmXl5KiW7-OS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"executionInfo":{"status":"ok","timestamp":1598800553040,"user_tz":-600,"elapsed":29227,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"9093a005-f992-4ee4-cc32-7d213f01034b"},"source":["\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iOWGfR8c6QvZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"ok","timestamp":1598800686114,"user_tz":-600,"elapsed":1776,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"e541c63b-1188-4cfe-c192-b651097d1f10"},"source":["transform = transforms.Compose([\n","                                transforms.Resize((32, 32)),\n","                                transforms.ToTensor()\n","])\n","\n","flat_img = 3072  #32Ã—32Ã—3 --size of flattened image\n","\n","img = Image.open('/content/drive/My Drive/bird.jpg')\n","real_img = transform(img)\n","\n","torch.manual_seed(2)\n","fake_img = torch.rand(1, 100)\n","\n","plt.imshow(np.transpose(real_img.numpy(), (1, 2, 0)))\n","print(real_img.size())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([3, 32, 32])\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"5kYnuPCN8-oa","colab_type":"code","colab":{}},"source":["class Discriminator(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear = nn.Sequential(\n","        nn.Linear(flat_img, 10000),\n","        nn.ReLU(),\n","        nn.Linear(10000, 1),\n","        nn.Sigmoid()\n","    )\n","\n","  def forward(self, img):\n","    img = img.view(1, -1)\n","    out = self.linear(img)\n","\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PoZJV3tA9B5X","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear = nn.Sequential(\n","        nn.Linear(100, 10000),\n","        nn.LeakyReLU(),\n","        nn.Linear(10000, 4000),\n","        nn.LeakyReLU(),\n","        nn.Linear(4000, flat_img)\n","    )\n","\n","  def forward(self, latent_space):\n","    latent_space = latent_space.view(1, -1)\n","    out = self.linear(latent_space)\n","\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9rHcJdeX9INE","colab_type":"code","colab":{}},"source":["device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","discr = Discriminator().to(device)\n","gen = Generator().to(device)\n","\n","opt_d = optim.SGD(discr.parameters(), lr=0.001, momentum=0.9)\n","opt_g = optim.SGD(gen.parameters(), lr=0.001, momentum=0.9)\n","\n","criterion = nn.BCELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0VO7kYv9Mqt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["78065a500c4842249960dcf5185ed5d5","ddc1d234ad604718852d0367068a55db","cb359bc92c934a53b064c9722eaae8a4","156aeecaac7c43ba8090ebab69ab33d5","4d49299a5839457382aa7fbd365dc5a5","733d1bdfa5fd47bf8bd44461cdf5f668","30fb3a57161d42d6a32c2335fb6e9c36","d999bcb1d8b74f1abdd256e12bef9e98"]},"executionInfo":{"status":"ok","timestamp":1598801906118,"user_tz":-600,"elapsed":1036546,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"26f9c09a-686b-4edb-f3ea-d31a8fed6a6f"},"source":["epochs = 500\n","discr_e = 4 \n","gen_e = 3\n","\n","#whole model training starts here\n","for epoch in tqdm(range(epochs), total=epochs):\n","\n","  #discriminator training\n","  for k in range(discr_e):\n","    opt_d.zero_grad()\n","\n","    out_d1 = discr(real_img.to(device))\n","    #loss for real image\n","    loss_d1 = criterion(out_d1, torch.ones((1, 1)).to(device))\n","    loss_d1.backward()\n","\n","    out_d2 = gen(fake_img.to(device)).detach()\n","    #loss for fake image\n","    loss_d2 = criterion(discr(out_d2.to(device)), torch.zeros((1, 1)).to(device))\n","    loss_d2.backward()\n","\n","    opt_d.step()\n","\n","  #generator training\n","  for i in range(gen_e):\n","    opt_g.zero_grad()\n","    \n","    out_g = gen(fake_img.to(device))\n","    \n","    #Binary cross entropy loss\n","    #loss_g =  criterion(discr(out_g.to(device)), torch.ones(1, 1).to(device))\n","\n","    #----Loss function in the GAN paper\n","    #[log(1 - D(G(z)))]\n","    loss_g = torch.log(1.0 - (discr(out_g.to(device)))) \n","    loss_g.backward()\n","\n","    opt_g.step()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78065a500c4842249960dcf5185ed5d5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NNECPWGWBZX1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":271},"executionInfo":{"status":"ok","timestamp":1598801946853,"user_tz":-600,"elapsed":1090,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"27a23f21-3d31-4794-d7e3-ea634412ac88"},"source":["plt.figure(figsize=(8, 8))\n","\n","plt.subplot(1, 2, 1)\n","plt.title(\"Generated Image\")\n","plt.xticks([])\n","plt.yticks([])\n","plt.imshow(np.transpose(out_g.resize(3, 32, 32).cpu().detach().numpy(), (1, 2, 0)))\n","\n","plt.subplot(1, 2, 2)\n","plt.title(\"Original Image\")\n","plt.xticks([])\n","plt.yticks([])\n","plt.imshow(np.transpose(real_img.numpy(), (1, 2, 0)))\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x576 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"E8AYRt3_hoCX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598843975778,"user_tz":-600,"elapsed":3480,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"fffd32a1-ba68-44ea-c39a-eaf6a4465f09"},"source":["!pip3 install utils"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_YrJZ84th2_F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1598846031320,"user_tz":-600,"elapsed":2123,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"1a9c7b36-7b3a-458b-a09a-0f2b2454bbcd"},"source":["!pip3 install utils.coco.coco"],"execution_count":13,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement utils.coco.coco (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for utils.coco.coco\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xwf-_ndbhNUP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"status":"error","timestamp":1598845638101,"user_tz":-600,"elapsed":1077,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"1f0d02fe-ddfc-4728-ada1-7ccd37fa2e5a"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","from utils.coco.coco import COCO\n","from utils.vocabulary import Vocabulary\n","\n","class DataSet(object):\n","    def __init__(self,\n","                 image_ids,\n","                 image_files,\n","                 batch_size,\n","                 word_idxs=None,\n","                 masks=None,\n","                 is_train=False,\n","                 shuffle=False):\n","        self.image_ids = np.array(image_ids)\n","        self.image_files = np.array(image_files)\n","        self.word_idxs = np.array(word_idxs)\n","        self.masks = np.array(masks)\n","        self.batch_size = batch_size\n","        self.is_train = is_train\n","        self.shuffle = shuffle\n","        self.setup()\n","\n","    def setup(self):\n","        \"\"\" Setup the dataset. \"\"\"\n","        self.count = len(self.image_ids)\n","        self.num_batches = int(np.ceil(self.count * 1.0 / self.batch_size))\n","        self.fake_count = self.num_batches * self.batch_size - self.count\n","        self.idxs = list(range(self.count))\n","        self.reset()\n","\n","    def reset(self):\n","        \"\"\" Reset the dataset. \"\"\"\n","        self.current_idx = 0\n","        if self.shuffle:\n","            np.random.shuffle(self.idxs)\n","\n","    def next_batch(self):\n","        \"\"\" Fetch the next batch. \"\"\"\n","        assert self.has_next_batch()\n","\n","        if self.has_full_next_batch():\n","            start, end = self.current_idx, \\\n","                         self.current_idx + self.batch_size\n","            current_idxs = self.idxs[start:end]\n","        else:\n","            start, end = self.current_idx, self.count\n","            current_idxs = self.idxs[start:end] + \\\n","                           list(np.random.choice(self.count, self.fake_count))\n","\n","        image_files = self.image_files[current_idxs]\n","        if self.is_train:\n","            word_idxs = self.word_idxs[current_idxs]\n","            masks = self.masks[current_idxs]\n","            self.current_idx += self.batch_size\n","            return image_files, word_idxs, masks\n","        else:\n","            self.current_idx += self.batch_size\n","            return image_files\n","\n","    def has_next_batch(self):\n","        \"\"\" Determine whether there is a batch left. \"\"\"\n","        return self.current_idx < self.count\n","\n","    def has_full_next_batch(self):\n","        \"\"\" Determine whether there is a full batch left. \"\"\"\n","        return self.current_idx + self.batch_size <= self.count\n","\n","def prepare_train_data(config):\n","    \"\"\" Prepare the data for training the model. \"\"\"\n","    coco = COCO(config.train_caption_file)\n","    coco.filter_by_cap_len(config.max_caption_length)\n","\n","    print(\"Building the vocabulary...\")\n","    vocabulary = Vocabulary(config.vocabulary_size)\n","    if not os.path.exists(config.vocabulary_file):\n","        vocabulary.build(coco.all_captions())\n","        vocabulary.save(config.vocabulary_file)\n","    else:\n","        vocabulary.load(config.vocabulary_file)\n","    print(\"Vocabulary built.\")\n","    print(\"Number of words = %d\" %(vocabulary.size))\n","\n","    coco.filter_by_words(set(vocabulary.words))\n","\n","    print(\"Processing the captions...\")\n","    if not os.path.exists(config.temp_annotation_file):\n","        captions = [coco.anns[ann_id]['caption'] for ann_id in coco.anns]\n","        image_ids = [coco.anns[ann_id]['image_id'] for ann_id in coco.anns]\n","        image_files = [os.path.join(config.train_image_dir,\n","                                    coco.imgs[image_id]['file_name'])\n","                                    for image_id in image_ids]\n","        annotations = pd.DataFrame({'image_id': image_ids,\n","                                    'image_file': image_files,\n","                                    'caption': captions})\n","        annotations.to_csv(config.temp_annotation_file)\n","    else:\n","        annotations = pd.read_csv(config.temp_annotation_file)\n","        captions = annotations['caption'].values\n","        image_ids = annotations['image_id'].values\n","        image_files = annotations['image_file'].values\n","\n","    if not os.path.exists(config.temp_data_file):\n","        word_idxs = []\n","        masks = []\n","        for caption in tqdm(captions):\n","            current_word_idxs_ = vocabulary.process_sentence(caption)\n","            current_num_words = len(current_word_idxs_)\n","            current_word_idxs = np.zeros(config.max_caption_length,\n","                                         dtype = np.int32)\n","            current_masks = np.zeros(config.max_caption_length)\n","            current_word_idxs[:current_num_words] = np.array(current_word_idxs_)\n","            current_masks[:current_num_words] = 1.0\n","            word_idxs.append(current_word_idxs)\n","            masks.append(current_masks)\n","        word_idxs = np.array(word_idxs)\n","        masks = np.array(masks)\n","        data = {'word_idxs': word_idxs, 'masks': masks}\n","        np.save(config.temp_data_file, data)\n","    else:\n","        data = np.load(config.temp_data_file).item()\n","        word_idxs = data['word_idxs']\n","        masks = data['masks']\n","    print(\"Captions processed.\")\n","    print(\"Number of captions = %d\" %(len(captions)))\n","\n","    print(\"Building the dataset...\")\n","    dataset = DataSet(image_ids,\n","                      image_files,\n","                      config.batch_size,\n","                      word_idxs,\n","                      masks,\n","                      True,\n","                      True)\n","    print(\"Dataset built.\")\n","    return dataset\n","\n","def prepare_eval_data(config):\n","    \"\"\" Prepare the data for evaluating the model. \"\"\"\n","    coco = COCO(config.eval_caption_file)\n","    image_ids = list(coco.imgs.keys())\n","    image_files = [os.path.join(config.eval_image_dir,\n","                                coco.imgs[image_id]['file_name'])\n","                                for image_id in image_ids]\n","\n","    print(\"Building the vocabulary...\")\n","    if os.path.exists(config.vocabulary_file):\n","        vocabulary = Vocabulary(config.vocabulary_size,\n","                                config.vocabulary_file)\n","    else:\n","        vocabulary = build_vocabulary(config)\n","    print(\"Vocabulary built.\")\n","    print(\"Number of words = %d\" %(vocabulary.size))\n","\n","    print(\"Building the dataset...\")\n","    dataset = DataSet(image_ids, image_files, config.batch_size)\n","    print(\"Dataset built.\")\n","    return coco, dataset, vocabulary\n","\n","def prepare_test_data(config):\n","    \"\"\" Prepare the data for testing the model. \"\"\"\n","    files = os.listdir(config.test_image_dir)\n","    image_files = [os.path.join(config.test_image_dir, f) for f in files\n","        if f.lower().endswith('.jpg') or f.lower().endswith('.jpeg')]\n","    image_ids = list(range(len(image_files)))\n","\n","    print(\"Building the vocabulary...\")\n","    if os.path.exists(config.vocabulary_file):\n","        vocabulary = Vocabulary(config.vocabulary_size,\n","                                config.vocabulary_file)\n","    else:\n","        vocabulary = build_vocabulary(config)\n","    print(\"Vocabulary built.\")\n","    print(\"Number of words = %d\" %(vocabulary.size))\n","\n","    print(\"Building the dataset...\")\n","    dataset = DataSet(image_ids, image_files, config.batch_size)\n","    print(\"Dataset built.\")\n","    return dataset, vocabulary\n","\n","def build_vocabulary(config):\n","    \"\"\" Build the vocabulary from the training data and save it to a file. \"\"\"\n","    coco = COCO(config.train_caption_file)\n","    coco.filter_by_cap_len(config.max_caption_length)\n","\n","    vocabulary = Vocabulary(config.vocabulary_size)\n","    vocabulary.build(coco.all_captions())\n","    vocabulary.save(config.vocabulary_file)\n","    return vocabulary"],"execution_count":10,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-d066656963dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.coco'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"gVmz-a33qL-i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1598863007499,"user_tz":-600,"elapsed":1805,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"95cafc5a-2778-4141-edaa-0374a9f36a23"},"source":["!pip3 install lib.defaultFlags"],"execution_count":17,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement lib.defaultFlags (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for lib.defaultFlags\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hVFfIX85qENK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"status":"error","timestamp":1598862945845,"user_tz":-600,"elapsed":3182,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"a3e09cce-c5f0-40d5-ddf0-d22207b43a85"},"source":["import numpy as np\n","import os\n","import h5py\n","import tensorflow as tf\n","\n","from lib.defaultFlags import defaultFlags\n","from lib.TEResNet import TEResNet\n","from lib.TEGAN import TEGAN\n","from lib.readTFRecord import getTFRecordFilenamesIn \n","\n","FLAGS = defaultFlags()\n","\n","# Check the output_dir is given\n","if FLAGS.output_dir is None:\n","    raise ValueError('The output directory is needed')\n","\n","# Check the output directory to save the checkpoint\n","if not os.path.exists(FLAGS.output_dir):\n","    os.mkdir(FLAGS.output_dir)\n","\n","# Check the summary_dir is given\n","if FLAGS.summary_dir is None:\n","    raise ValueError('The summary directory is needed')\n","\n","# Check the summary directory to save the event\n","if not os.path.exists(FLAGS.summary_dir):\n","    os.mkdir(FLAGS.summary_dir)\n","\n","filenames_train = getTFRecordFilenamesIn(FLAGS.train_dir)\n","if len(filenames_train) % FLAGS.batch_size != 0:\n","    raise RuntimeError(\"Tread with CAUTION! Training dataset size is not a multiple of batch_size\")\n","\n","filenames_dev = getTFRecordFilenamesIn(FLAGS.dev_dir)\n","if len(filenames_dev) % FLAGS.batch_size != 0:\n","    raise RuntimeError(\"Tread with CAUTION! Training dataset size is not a multiple of batch_size\")\n","\n","if FLAGS.task == 'TEResNet':\n","    net = TEResNet(filenames_train, filenames_dev, FLAGS)\n","elif FLAGS.task == 'TEGAN':\n","    net = TEGAN(filenames_train, filenames_dev, FLAGS)\n","else:\n","    raise ValueError('Need to specify FLAGS.task to be TEResNet or TEGAN')\n","\n","with tf.Session() as sess:\n","    print(\"--------------- {} mode -----------------\".format(FLAGS.mode))\n","    net.initialize(sess)    \n","    print(\"Finished initializing :D\")\n","\n","    if FLAGS.mode == 'train':\n","        net.optimize(sess)\n","\n","    elif FLAGS.mode == 'test':\n","        filenames_test = getTFRecordFilenamesIn(FLAGS.test_dir)\n","        output = net.evaluate(sess, [ filenames_test[0] ])\n","        losses = net.evaluate_losses(sess, filenames_test)\n","\n","        filename_out = os.path.basename(filenames_test[0]).replace('.tfrecord','.h5')\n","        print(\"Saving test data to {}\".format(os.path.join(FLAGS.output_dir, filename_out)))\n","        h5f = h5py.File(os.path.join(FLAGS.output_dir, filename_out), 'w')\n","        h5f.create_dataset('HR', data=output[0][0])\n","        h5f.create_dataset('LR', data=output[0][1])\n","        h5f.create_dataset('output', data=output[0][2])\n","        h5f.close()\n","\n","        for loss, val in losses.items():\n","            print(\"{0:26s} = {1:0.6f}\".format(loss,np.mean(val)))\n","    print(\"--------------- ******** -----------------\")"],"execution_count":14,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-44aca2e31ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultFlags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultFlags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEResNet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTEResNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEGAN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTEGAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"v9HLwa25stja","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1598863671953,"user_tz":-600,"elapsed":2175,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"6c0f0346-2da2-486b-8e12-9e9dc4882489"},"source":["!pip3 install pylearn3"],"execution_count":25,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement pylearn3 (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for pylearn3\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4WL2U8NNuDvh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"status":"ok","timestamp":1598863991714,"user_tz":-600,"elapsed":3547,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"2beeb2dc-a010-4673-e383-5b48641238b7"},"source":["! pip3 install theano"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (1.0.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano) (1.15.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano) (1.18.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VE7zxRP9uSxw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1598864081549,"user_tz":-600,"elapsed":2325,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"bc423497-5aa3-4822-eaa6-36901b3e3c44"},"source":["!pip3 install pylearn2.linear.conv2d_c01b"],"execution_count":27,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement pylearn2.linear.conv2d_c01b (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for pylearn2.linear.conv2d_c01b\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vpMqVfMjuc_F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1598864206794,"user_tz":-600,"elapsed":2124,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"d9e10a0f-722e-4d0d-d150-31961e3453e8"},"source":["!pip3 install pylearn3"],"execution_count":30,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement pylearn3 (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for pylearn3\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sVSFPQ2RrGlm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"status":"error","timestamp":1598863578694,"user_tz":-600,"elapsed":1755,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"83e37f1f-57d4-47c4-b378-d95fb0fb6421"},"source":["import numpy as np\n","\n","from theano.compat import OrderedDict\n","from theano import tensor as T\n","\n","from pylearn2.linear.conv2d_c01b import make_random_conv2D\n","from pylearn2.models import Model\n","from pylearn2.models.maxout import check_cuda # TODO: import from original path\n","from pylearn2.models.mlp import Layer\n","#from pylearn2.models.maxout import py_integer_types # TODO: import from orig path\n","from pylearn2.space import Conv2DSpace\n","from pylearn2.utils import sharedX\n","\n","logger = logging.getLogger(__name__)\n","\n","class Deconv(Layer):\n","    def __init__(self,\n","                 num_channels,\n","                 kernel_shape,\n","                 layer_name,\n","                 irange=None,\n","                 init_bias=0.,\n","                 W_lr_scale=None,\n","                 b_lr_scale=None,\n","                 pad_out=0,\n","                 fix_kernel_shape=False,\n","                 partial_sum=1,\n","                 tied_b=False,\n","                 max_kernel_norm=None,\n","                 output_stride=(1, 1)):\n","        check_cuda(str(type(self)))\n","        super(Deconv, self).__init__()\n","\n","        detector_channels = num_channels\n","\n","        self.__dict__.update(locals())\n","        del self.self\n","\n","    @functools.wraps(Model.get_lr_scalers)\n","    def get_lr_scalers(self):\n","\n","        if not hasattr(self, 'W_lr_scale'):\n","            self.W_lr_scale = None\n","\n","        if not hasattr(self, 'b_lr_scale'):\n","            self.b_lr_scale = None\n","\n","        rval = OrderedDict()\n","\n","        if self.W_lr_scale is not None:\n","            W, = self.transformer.get_params()\n","            rval[W] = self.W_lr_scale\n","\n","        if self.b_lr_scale is not None:\n","            rval[self.b] = self.b_lr_scale\n","\n","        return rval\n","\n","    def set_input_space(self, space):\n","        \"\"\"\n","        Tells the layer to use the specified input space.\n","        This resets parameters! The kernel tensor is initialized with the\n","        size needed to receive input from this space.\n","        Parameters\n","        ----------\n","        space : Space\n","            The Space that the input will lie in.\n","        \"\"\"\n","\n","        setup_deconv_detector_layer_c01b(layer=self,\n","                                  input_space=space,\n","                                  rng=self.mlp.rng)\n","\n","        rng = self.mlp.rng\n","\n","        detector_shape = self.detector_space.shape\n","\n","\n","        self.output_space = self.detector_space\n","\n","        logger.info('Output space: {0}'.format(self.output_space.shape))\n","\n","    def _modify_updates(self, updates):\n","        \"\"\"\n","        Replaces the values in `updates` if needed to enforce the options set\n","        in the __init__ method, including `max_kernel_norm`.\n","        Parameters\n","        ----------\n","        updates : OrderedDict\n","            A dictionary mapping parameters (including parameters not\n","            belonging to this model) to updated values of those parameters.\n","            The dictionary passed in contains the updates proposed by the\n","            learning algorithm. This function modifies the dictionary\n","            directly. The modified version will be compiled and executed\n","            by the learning algorithm.\n","        \"\"\"\n","\n","        if self.max_kernel_norm is not None:\n","            W, = self.transformer.get_params()\n","            if W in updates:\n","                updated_W = updates[W]\n","                row_norms = T.sqrt(T.sum(T.sqr(updated_W), axis=(0, 1, 2)))\n","                desired_norms = T.clip(row_norms, 0, self.max_kernel_norm)\n","                scales = desired_norms / (1e-7 + row_norms)\n","                updates[W] = (updated_W * scales.dimshuffle('x', 'x', 'x', 0))\n","\n","    @functools.wraps(Model.get_params)\n","    def get_params(self):\n","        assert self.b.name is not None\n","        W, = self.transformer.get_params()\n","        assert W.name is not None\n","        rval = self.transformer.get_params()\n","        assert not isinstance(rval, set)\n","        rval = list(rval)\n","        assert self.b not in rval\n","        rval.append(self.b)\n","        return rval\n","\n","    @functools.wraps(Layer.get_weight_decay)\n","    def get_weight_decay(self, coeff):\n","        if isinstance(coeff, str):\n","            coeff = float(coeff)\n","        assert isinstance(coeff, float) or hasattr(coeff, 'dtype')\n","        W, = self.transformer.get_params()\n","        return coeff * T.sqr(W).sum()\n","\n","    @functools.wraps(Layer.set_weights)\n","    def set_weights(self, weights):\n","        W, = self.transformer.get_params()\n","        W.set_value(weights)\n","\n","    @functools.wraps(Layer.set_biases)\n","    def set_biases(self, biases):\n","        self.b.set_value(biases)\n","\n","    @functools.wraps(Layer.get_biases)\n","    def get_biases(self):\n","        return self.b.get_value()\n","\n","    @functools.wraps(Model.get_weights_topo)\n","    def get_weights_topo(self):\n","        return self.transformer.get_weights_topo()\n","\n","    @functools.wraps(Layer.get_monitoring_channels)\n","    def get_layer_monitoring_channels(self, state_below=None, state=None, targets=None):\n","\n","        W, = self.transformer.get_params()\n","\n","        assert W.ndim == 4\n","\n","        sq_W = T.sqr(W)\n","\n","        row_norms = T.sqrt(sq_W.sum(axis=(0, 1, 2)))\n","\n","        P = state\n","\n","        rval = OrderedDict()\n","\n","        vars_and_prefixes = [(P, '')]\n","\n","        for var, prefix in vars_and_prefixes:\n","            if not hasattr(var, 'ndim') or var.ndim != 4:\n","                print (\"expected 4D tensor, got \")\n","                print (var)\n","                print (type(var))\n","                if isinstance(var, tuple):\n","                    print (\"tuple length: \", len(var))\n","                assert False\n","            v_max = var.max(axis=(1, 2, 3))\n","            v_min = var.min(axis=(1, 2, 3))\n","            v_mean = var.mean(axis=(1, 2, 3))\n","            v_range = v_max - v_min\n","\n","            # max_x.mean_u is \"the mean over *u*nits of the max over\n","            # e*x*amples\" The x and u are included in the name because\n","            # otherwise its hard to remember which axis is which when reading\n","            # the monitor I use inner.outer rather than outer_of_inner or\n","            # something like that because I want mean_x.* to appear next to\n","            # each other in the alphabetical list, as these are commonly\n","            # plotted together\n","            for key, val in [('max_x.max_u',    v_max.max()),\n","                             ('max_x.mean_u',   v_max.mean()),\n","                             ('max_x.min_u',    v_max.min()),\n","                             ('min_x.max_u',    v_min.max()),\n","                             ('min_x.mean_u',   v_min.mean()),\n","                             ('min_x.min_u',    v_min.min()),\n","                             ('range_x.max_u',  v_range.max()),\n","                             ('range_x.mean_u', v_range.mean()),\n","                             ('range_x.min_u',  v_range.min()),\n","                             ('mean_x.max_u',   v_mean.max()),\n","                             ('mean_x.mean_u',  v_mean.mean()),\n","                             ('mean_x.min_u',   v_mean.min())]:\n","                rval[prefix+key] = val\n","\n","        rval.update(OrderedDict([('kernel_norms_min',  row_norms.min()),\n","                            ('kernel_norms_mean', row_norms.mean()),\n","                            ('kernel_norms_max',  row_norms.max()), ]))\n","\n","        return rval\n","\n","    @functools.wraps(Layer.fprop)\n","    def fprop(self, state_below):\n","        check_cuda(str(type(self)))\n","\n","        self.input_space.validate(state_below)\n","\n","        z = self.transformer.lmul_T(state_below)\n","\n","        self.output_space.validate(z)\n","\n","        if not hasattr(self, 'tied_b'):\n","            self.tied_b = False\n","        if self.tied_b:\n","            b = self.b.dimshuffle(0, 'x', 'x', 'x')\n","        else:\n","            b = self.b.dimshuffle(0, 1, 2, 'x')\n","\n","        return z + b\n","\n","\n","\n","def setup_deconv_detector_layer_c01b(layer, input_space, rng, irange=\"not specified\"):\n","    \"\"\"\n","    layer. This function sets up only the detector layer.\n","    Does the following:\n","    * raises a RuntimeError if cuda is not available\n","    * sets layer.input_space to input_space\n","    * sets up addition of dummy channels for compatibility with cuda-convnet:\n","      - layer.dummy_channels: # of dummy channels that need to be added\n","        (You might want to check this and raise an Exception if it's not 0)\n","      - layer.dummy_space: The Conv2DSpace representing the input with dummy\n","        channels added\n","    * sets layer.detector_space to the space for the detector layer\n","    * sets layer.transformer to be a Conv2D instance\n","    * sets layer.b to the right value\n","    Parameters\n","    ----------\n","    layer : object\n","        Any python object that allows the modifications described below and\n","        has the following attributes:\n","          * pad : int describing amount of zero padding to add\n","          * kernel_shape : 2-element tuple or list describing spatial shape of\n","            kernel\n","          * fix_kernel_shape : bool, if true, will shrink the kernel shape to\n","            make it feasible, as needed (useful for hyperparameter searchers)\n","          * detector_channels : The number of channels in the detector layer\n","          * init_bias : numeric constant added to a tensor of zeros to\n","            initialize the bias\n","          * tied_b : If true, biases are shared across all spatial locations\n","    input_space : WRITEME\n","        A Conv2DSpace to be used as input to the layer\n","    rng : WRITEME\n","        A numpy RandomState or equivalent\n","    \"\"\"\n","\n","    if irange != \"not specified\":\n","        raise AssertionError(\n","            \"There was a bug in setup_detector_layer_c01b.\"\n","            \"It uses layer.irange instead of the irange parameter to the \"\n","            \"function. The irange parameter is now disabled by this \"\n","            \"AssertionError, so that this error message can alert you that \"\n","            \"the bug affected your code and explain why the interface is \"\n","            \"changing. The irange parameter to the function and this \"\n","            \"error message may be removed after April 21, 2014.\"\n","        )\n","\n","    # Use \"self\" to refer to layer from now on, so we can pretend we're\n","    # just running in the set_input_space method of the layer\n","    self = layer\n","\n","    # Make sure cuda is available\n","    check_cuda(str(type(self)))\n","\n","    # Validate input\n","    if not isinstance(input_space, Conv2DSpace):\n","        raise TypeError(\"The input to a convolutional layer should be a \"\n","                        \"Conv2DSpace, but layer \" + self.layer_name + \" got \" +\n","                        str(type(self.input_space)))\n","\n","    if not hasattr(self, 'detector_channels'):\n","        raise ValueError(\"layer argument must have a 'detector_channels' \"\n","                         \"attribute specifying how many channels to put in \"\n","                         \"the convolution kernel stack.\")\n","\n","    # Store the input space\n","    self.input_space = input_space\n","\n","    # Make sure number of channels is supported by cuda-convnet\n","    # (multiple of 4 or <= 3)\n","    # If not supported, pad the input with dummy channels\n","    ch = self.detector_channels\n","    rem = ch % 4\n","    if ch > 3 and rem != 0:\n","        raise NotImplementedError(\"Need to do dummy channels on the output\")\n","    #    self.dummy_channels = 4 - rem\n","    #else:\n","    #    self.dummy_channels = 0\n","    #self.dummy_space = Conv2DSpace(\n","    #    shape=input_space.shape,\n","    #    channels=input_space.num_channels + self.dummy_channels,\n","    #    axes=('c', 0, 1, 'b')\n","    #)\n","\n","    if hasattr(self, 'output_stride'):\n","        kernel_stride = self.output_stride\n","    else:\n","        assert False # not sure if I got the name right, remove this assert if I did\n","        kernel_stride = [1, 1]\n","\n","\n","    #o_sh = int(np.ceil((i_sh + 2. * self.pad - k_sh) / float(k_st))) + 1\n","    #o_sh -1 = np.ceil((i_sh + 2. * self.pad - k_sh) / float(k_st))\n","    #inv_ceil(o_sh -1) = (i_sh + 2. * self.pad - k_sh) / float(k_st)\n","    #float(k_st) inv_cel(o_sh -1) = (i_sh + 2 * self.pad -k_sh)\n","    # i_sh = k_st inv_ceil(o_sh-1) - 2 * self.pad + k_sh\n","\n","    output_shape = \\\n","        [k_st * (i_sh - 1) - 2 * self.pad_out + k_sh\n","         for i_sh, k_sh, k_st in zip(self.input_space.shape,\n","                                     self.kernel_shape, kernel_stride)]\n","\n","\n","    if self.input_space.num_channels < 16:\n","        raise ValueError(\"Cuda-convnet requires the input to lmul_T to have \"\n","                         \"at least 16 channels.\")\n","\n","    self.detector_space = Conv2DSpace(shape=output_shape,\n","                                      num_channels=self.detector_channels,\n","                                      axes=('c', 0, 1, 'b'))\n","\n","    if hasattr(self, 'partial_sum'):\n","        partial_sum = self.partial_sum\n","    else:\n","        partial_sum = 1\n","\n","    if hasattr(self, 'sparse_init') and self.sparse_init is not None:\n","        self.transformer = \\\n","            checked_call(make_sparse_random_conv2D,\n","                         OrderedDict([('num_nonzero', self.sparse_init),\n","                                      ('input_space', self.detector_space),\n","                                      ('output_space', self.input_space),\n","                                      ('kernel_shape', self.kernel_shape),\n","                                      ('pad', self.pad),\n","                                      ('partial_sum', partial_sum),\n","                                      ('kernel_stride', kernel_stride),\n","                                      ('rng', rng)]))\n","    else:\n","        self.transformer = make_random_conv2D(\n","            irange=self.irange,\n","            input_axes=self.detector_space.axes,\n","            output_axes=self.input_space.axes,\n","            input_channels=self.detector_space.num_channels,\n","            output_channels=self.input_space.num_channels,\n","            kernel_shape=self.kernel_shape,\n","            pad=self.pad_out,\n","            partial_sum=partial_sum,\n","            kernel_stride=kernel_stride,\n","            rng=rng,\n","            input_shape=self.detector_space.shape\n","        )\n","\n","    W, = self.transformer.get_params()\n","    W.name = self.layer_name + '_W'\n","\n","    if self.tied_b:\n","        self.b = sharedX(np.zeros(self.detector_space.num_channels) +\n","                         self.init_bias)\n","    else:\n","        self.b = sharedX(self.detector_space.get_origin() + self.init_bias)\n","    self.b.name = self.layer_name + '_b'\n","\n","    logger.info('Input shape: {0}'.format(self.input_space.shape))\n","    print (layer.layer_name + ' detector space: {0}'.format(self.detector_space.shape))"],"execution_count":23,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-57704d2efba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpylearn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_c01b\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_random_conv2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylearn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylearn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_cuda\u001b[0m \u001b[0;31m# TODO: import from original path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pylearn2'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"0iiexjrywEzM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1598864550339,"user_tz":-600,"elapsed":3999,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"0d18732a-1f49-4048-d9d5-fcb9c91b5c60"},"source":["!pip3 install tokenization"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Collecting tokenization\n","  Downloading https://files.pythonhosted.org/packages/60/eb/8e1756b0ce07dab8b0f8267019738d0e4ea2fc8f6eb3fe4d433daac38a1d/tokenization-1.0.7-py3-none-any.whl\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from tokenization) (2019.12.20)\n","Installing collected packages: tokenization\n","Successfully installed tokenization-1.0.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I8iPyS0ZwaMb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598864615591,"user_tz":-600,"elapsed":3424,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"33bb4648-7a29-4cc4-c436-640defe54d4c"},"source":["!pip3 install utils"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WgIVSDIgxY-s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":257},"executionInfo":{"status":"ok","timestamp":1598864877015,"user_tz":-600,"elapsed":7919,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"9eb6d803-41aa-4b7a-c10b-97ed14a9976d"},"source":["!pip3 install git+git://github.com/keras-team/keras.git --upgrade --no-deps"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Collecting git+git://github.com/keras-team/keras.git\n","  Cloning git://github.com/keras-team/keras.git to /tmp/pip-req-build-mtzl6rea\n","  Running command git clone -q git://github.com/keras-team/keras.git /tmp/pip-req-build-mtzl6rea\n","Building wheels for collected packages: Keras\n","  Building wheel for Keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Keras: filename=Keras-2.4.3-cp36-none-any.whl size=36257 sha256=50364484f89cb37a9d5b32500c4e1c40e110d4fa391d173cbaccf3bd2348e95d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-8ltirtbt/wheels/e6/02/ad/5e8e1a5824af71082e2260fe7e2eaa1b745c34706e6ff0a14b\n","Successfully built Keras\n","Installing collected packages: Keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed Keras-2.4.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8zx2b3VBvzhI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"error","timestamp":1598864883907,"user_tz":-600,"elapsed":1167,"user":{"displayName":"Masroor Fattah Bin Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwqCrajm_0dRO1GA56MUyetk43mz3WM9T6pLBa=s64","userId":"01414959232899052070"}},"outputId":"c0c31048-cea5-4037-d481-c911cd939658"},"source":["import argparse\n","import multiprocessing\n","import os\n","import random\n","import time\n","import tensorflow.compat.v1 as tf\n","\n","import tokenization\n","import utils\n","\n","\n","def create_int_feature(values):\n","  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n","  return feature\n","\n","\n","class ExampleBuilder(object):\n","  \"\"\"Given a stream of input text, creates pretraining examples.\"\"\"\n","\n","  def __init__(self, tokenizer, max_length):\n","    self._tokenizer = tokenizer\n","    self._current_sentences = []\n","    self._current_length = 0\n","    self._max_length = max_length\n","    self._target_length = max_length\n","\n","  def add_line(self, line):\n","    \"\"\"Adds a line of text to the current example being built.\"\"\"\n","    line = line.strip().replace(\"\\n\", \" \")\n","    if (not line) and self._current_length != 0:  # empty lines separate docs\n","      return self._create_example()\n","    bert_tokens = self._tokenizer.tokenize(line)\n","    bert_tokids = self._tokenizer.convert_tokens_to_ids(bert_tokens)\n","    self._current_sentences.append(bert_tokids)\n","    self._current_length += len(bert_tokids)\n","    if self._current_length >= self._target_length:\n","      return self._create_example()\n","    return None\n","\n","  def _create_example(self):\n","    \"\"\"Creates a pre-training example from the current list of sentences.\"\"\"\n","    # small chance to only have one segment as in classification tasks\n","    if random.random() < 0.1:\n","      first_segment_target_length = 100000\n","    else:\n","      # -3 due to not yet having [CLS]/[SEP] tokens in the input text\n","      first_segment_target_length = (self._target_length - 3) // 2\n","\n","    first_segment = []\n","    second_segment = []\n","    for sentence in self._current_sentences:\n","      # the sentence goes to the first segment if (1) the first segment is\n","      # empty, (2) the sentence doesn't put the first segment over length or\n","      # (3) 50% of the time when it does put the first segment over length\n","      if (len(first_segment) == 0 or\n","          len(first_segment) + len(sentence) < first_segment_target_length or\n","          (len(second_segment) == 0 and\n","           len(first_segment) < first_segment_target_length and\n","           random.random() < 0.5)):\n","        first_segment += sentence\n","      else:\n","        second_segment += sentence\n","\n","    # trim to max_length while accounting for not-yet-added [CLS]/[SEP] tokens\n","    first_segment = first_segment[:self._max_length - 2]\n","    second_segment = second_segment[:max(0, self._max_length -\n","                                         len(first_segment) - 3)]\n","\n","    # prepare to start building the next example\n","    self._current_sentences = []\n","    self._current_length = 0\n","    # small chance for random-length instead of max_length-length example\n","    if random.random() < 0.05:\n","      self._target_length = random.randint(5, self._max_length)\n","    else:\n","      self._target_length = self._max_length\n","\n","    return self._make_tf_example(first_segment, second_segment)\n","\n","  def _make_tf_example(self, first_segment, second_segment):\n","    \"\"\"Converts two \"segments\" of text into a tf.train.Example.\"\"\"\n","    vocab = self._tokenizer.vocab\n","    input_ids = [vocab[\"[CLS]\"]] + first_segment + [vocab[\"[SEP]\"]]\n","    segment_ids = [0] * len(input_ids)\n","    if second_segment:\n","      input_ids += second_segment + [vocab[\"[SEP]\"]]\n","      segment_ids += [1] * (len(second_segment) + 1)\n","    input_mask = [1] * len(input_ids)\n","    input_ids += [0] * (self._max_length - len(input_ids))\n","    input_mask += [0] * (self._max_length - len(input_mask))\n","    segment_ids += [0] * (self._max_length - len(segment_ids))\n","    tf_example = tf.train.Example(features=tf.train.Features(feature={\n","        \"input_ids\": create_int_feature(input_ids),\n","        \"input_mask\": create_int_feature(input_mask),\n","        \"segment_ids\": create_int_feature(segment_ids)\n","    }))\n","    return tf_example\n","\n","\n","class ExampleWriter(object):\n","  \"\"\"Writes pre-training examples to disk.\"\"\"\n","\n","  def __init__(self, job_id, vocab_file, output_dir, max_seq_length,\n","               num_jobs, blanks_separate_docs, do_lower_case,\n","               num_out_files=1000):\n","    self._blanks_separate_docs = blanks_separate_docs\n","    tokenizer = tokenization.FullTokenizer(\n","        vocab_file=vocab_file,\n","        do_lower_case=do_lower_case)\n","    self._example_builder = ExampleBuilder(tokenizer, max_seq_length)\n","    self._writers = []\n","    for i in range(num_out_files):\n","      if i % num_jobs == job_id:\n","        output_fname = os.path.join(\n","            output_dir, \"pretrain_data.tfrecord-{:}-of-{:}\".format(\n","                i, num_out_files))\n","        self._writers.append(tf.io.TFRecordWriter(output_fname))\n","    self.n_written = 0\n","\n","  def write_examples(self, input_file):\n","    \"\"\"Writes out examples from the provided input file.\"\"\"\n","    with tf.io.gfile.GFile(input_file) as f:\n","      for line in f:\n","        line = line.strip()\n","        if line or self._blanks_separate_docs:\n","          example = self._example_builder.add_line(line)\n","          if example:\n","            self._writers[self.n_written % len(self._writers)].write(\n","                example.SerializeToString())\n","            self.n_written += 1\n","      example = self._example_builder.add_line(\"\")\n","      if example:\n","        self._writers[self.n_written % len(self._writers)].write(\n","            example.SerializeToString())\n","        self.n_written += 1\n","\n","  def finish(self):\n","    for writer in self._writers:\n","      writer.close()\n","\n","\n","def write_examples(job_id, args):\n","  \"\"\"A single process creating and writing out pre-processed examples.\"\"\"\n","\n","  def log(*args):\n","    msg = \" \".join(map(str, args))\n","    print(\"Job {}:\".format(job_id), msg)\n","\n","  log(\"Creating example writer\")\n","  example_writer = ExampleWriter(\n","      job_id=job_id,\n","      vocab_file=args.vocab_file,\n","      output_dir=args.output_dir,\n","      max_seq_length=args.max_seq_length,\n","      num_jobs=args.num_processes,\n","      blanks_separate_docs=args.blanks_separate_docs,\n","      do_lower_case=args.do_lower_case\n","  )\n","  log(\"Writing tf examples\")\n","  fnames = sorted(tf.io.gfile.listdir(args.corpus_dir))\n","  fnames = [f for (i, f) in enumerate(fnames)\n","            if i % args.num_processes == job_id]\n","  random.shuffle(fnames)\n","  start_time = time.time()\n","  for file_no, fname in enumerate(fnames):\n","    if file_no > 0:\n","      elapsed = time.time() - start_time\n","      log(\"processed {:}/{:} files ({:.1f}%), ELAPSED: {:}s, ETA: {:}s, \"\n","          \"{:} examples written\".format(\n","              file_no, len(fnames), 100.0 * file_no / len(fnames), int(elapsed),\n","              int((len(fnames) - file_no) / (file_no / elapsed)),\n","              example_writer.n_written))\n","    example_writer.write_examples(os.path.join(args.corpus_dir, fname))\n","  example_writer.finish()\n","  log(\"Done!\")\n","\n","\n","def main():\n","  parser = argparse.ArgumentParser(description=__doc__)\n","  parser.add_argument(\"--corpus-dir\", required=True,\n","                      help=\"Location of pre-training text files.\")\n","  parser.add_argument(\"--vocab-file\", required=True,\n","                      help=\"Location of vocabulary file.\")\n","  parser.add_argument(\"--output-dir\", required=True,\n","                      help=\"Where to write out the tfrecords.\")\n","  parser.add_argument(\"--max-seq-length\", default=128, type=int,\n","                      help=\"Number of tokens per example.\")\n","  parser.add_argument(\"--num-processes\", default=1, type=int,\n","                      help=\"Parallelize across multiple processes.\")\n","  parser.add_argument(\"--blanks-separate-docs\", default=True, type=bool,\n","                      help=\"Whether blank lines indicate document boundaries.\")\n","  parser.add_argument(\"--do-lower-case\", dest='do_lower_case',\n","                      action='store_true', help=\"Lower case input text.\")\n","  parser.add_argument(\"--no-lower-case\", dest='do_lower_case',\n","                      action='store_false', help=\"Don't lower case input text.\")\n","  parser.set_defaults(do_lower_case=True)\n","  args = parser.parse_args()\n","\n","  utils.rmkdir(args.output_dir)\n","  if args.num_processes == 1:\n","    write_examples(0, args)\n","  else:\n","    jobs = []\n","    for i in range(args.num_processes):\n","      job = multiprocessing.Process(target=write_examples, args=(i, args))\n","      jobs.append(job)\n","      job.start()\n","    for job in jobs:\n","      job.join()\n","\n","\n","if __name__ == \"__main__\":\n","  main()\n","\n","   "],"execution_count":40,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] --corpus-dir CORPUS_DIR --vocab-file\n","                             VOCAB_FILE --output-dir OUTPUT_DIR\n","                             [--max-seq-length MAX_SEQ_LENGTH]\n","                             [--num-processes NUM_PROCESSES]\n","                             [--blanks-separate-docs BLANKS_SEPARATE_DOCS]\n","                             [--do-lower-case] [--no-lower-case]\n","ipykernel_launcher.py: error: the following arguments are required: --corpus-dir, --vocab-file, --output-dir\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FPaGm3SfxwXI","colab_type":"code","colab":{}},"source":["import argparse\n","\n","import torch\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","import random\n","\n","from dcgan import Generator\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('-load_path', default='model/model_final.pth', help='Checkpoint to load path from')\n","parser.add_argument('-num_output', default=64, help='Number of generated outputs')\n","args = parser.parse_args()\n","\n","# Load the checkpoint file.\n","state_dict = torch.load(args.load_path)\n","\n","# Set the device to run on: GPU or CPU.\n","device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n","# Get the 'params' dictionary from the loaded state_dict.\n","params = state_dict['params']\n","\n","# Create the generator network.\n","netG = Generator(params).to(device)\n","# Load the trained generator weights.\n","netG.load_state_dict(state_dict['generator'])\n","print(netG)\n","\n","print(args.num_output)\n","# Get latent vector Z from unit normal distribution.\n","noise = torch.randn(int(args.num_output), params['nz'], 1, 1, device=device)\n","\n","# Turn off gradient calculation to speed up the process.\n","with torch.no_grad():\n","\t# Get generated image from the noise vector using\n","\t# the trained generator.\n","    generated_img = netG(noise).detach().cpu()\n","\n","# Display the generated image.\n","plt.axis(\"off\")\n","plt.title(\"Generated Images\")\n","plt.imshow(np.transpose(vutils.make_grid(generated_img, padding=2, normalize=True), (1,2,0)))\n","\n","plt.show()"],"execution_count":null,"outputs":[]}]}